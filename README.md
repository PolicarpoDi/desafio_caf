# ğŸš€ Pipeline de Dados - CAF

Este projeto implementa um pipeline de dados utilizando a arquitetura Medallion, com processamento em camadas (raw, bronze, silver e gold) e orquestraÃ§Ã£o via Apache Airflow.

## ğŸ”„ Fluxo de Dados

1. ğŸ“¥ Os dados sÃ£o extraÃ­dos dos arquivos JSON e salvos na camada raw
2. âœ… Os dados sÃ£o validados e movidos para a camada bronze
3. ğŸ”„ As transformaÃ§Ãµes sÃ£o aplicadas e os dados sÃ£o salvos na camada silver
4. ğŸ“¤ Os dados sÃ£o carregados no PostgreSQL na camada gold
5. ğŸ­ Todo o processo Ã© orquestrado pelo Airflow
6. ğŸ“Š Metadados sÃ£o gerados e armazenados em cada etapa

## ğŸ—ï¸ Arquitetura

### ğŸ“Š Diagrama do Pipeline

```mermaid
graph TD
    A[Extract] --> B[Raw Layer]
    B --> C[Validate]
    C --> D[Bronze Layer]
    D --> E[Transform]
    E --> F[Silver Layer]
    F --> G[Load]
    G --> H[Gold Layer]
    H --> I[Analytics Views]
```

### ğŸº Camadas do Medallion

1. **Raw Layer** ğŸ—ƒï¸
   - Primeira camada de ingestÃ£o
   - Armazena dados brutos sem transformaÃ§Ãµes
   - Formato: Parquet
   - MantÃ©m a estrutura original dos dados
   - Objetivo: Preservar dados originais para reprocessamento
   - **Arquivos de entrada**:
     - `data/users.json`
     - `data/customers.json`
     - `data/transactions.json`
   - **Arquivos de saÃ­da**:
     - `data/parquet/raw/users_[timestamp].parquet`
     - `data/parquet/raw/customers_[timestamp].parquet`
     - `data/parquet/raw/transactions_[timestamp].parquet`

2. **Bronze Layer** ğŸ†
   - Dados validados e limpos
   - ValidaÃ§Ã£o de schema usando Pydantic
   - VerificaÃ§Ã£o de qualidade de dados
   - Formato: Parquet
   - Objetivo: Garantir integridade dos dados
   - **Arquivos de entrada**: Arquivos Parquet da camada raw
   - **Arquivos de saÃ­da**:
     - `data/parquet/bronze/users_[timestamp].parquet`
     - `data/parquet/bronze/customers_[timestamp].parquet`
     - `data/parquet/bronze/transactions_[timestamp].parquet`

3. **Silver Layer** ğŸ¥ˆ
   - Dados transformados e enriquecidos
   - TransformaÃ§Ãµes de negÃ³cio
   - NormalizaÃ§Ã£o de dados
   - Formato: Parquet
   - Objetivo: Preparar dados para anÃ¡lise
   - **Arquivos de entrada**: Arquivos Parquet da camada bronze
   - **Arquivos de saÃ­da**:
     - `data/parquet/silver/users_[timestamp].parquet`
     - `data/parquet/silver/customers_[timestamp].parquet`
     - `data/parquet/silver/transactions_[timestamp].parquet`

4. **Gold Layer** ğŸ¥‡
   - Dados prontos para consumo
   - Modelo dimensional (Star Schema)
   - Views materializadas para anÃ¡lise
   - Formato: PostgreSQL
   - Objetivo: Suportar anÃ¡lises e relatÃ³rios
   - **Arquivos de entrada**: Arquivos Parquet da camada silver
   - **Arquivos de saÃ­da**:
     - Tabelas no PostgreSQL:
       - `gold.users`
       - `gold.customers`
       - `gold.transactions`

## ğŸ“ Estrutura do Projeto

```
.
â”œâ”€â”€ dags/                    # DAGs do Airflow
â”œâ”€â”€ data/                    # Dados processados
â”‚   â”œâ”€â”€ parquet/            # Arquivos Parquet
â”‚   â”‚   â”œâ”€â”€ raw/           # Camada Raw
â”‚   â”‚   â”œâ”€â”€ bronze/        # Camada Bronze
â”‚   â”‚   â””â”€â”€ silver/        # Camada Silver
â”‚   â””â”€â”€ metadata/          # Metadados do pipeline
â”œâ”€â”€ src/                    # CÃ³digo fonte
â”‚   â”œâ”€â”€ extractors/        # ExtraÃ§Ã£o de dados
â”‚   â”œâ”€â”€ transformers/      # TransformaÃ§Ã£o de dados
â”‚   â”œâ”€â”€ validators/        # ValidaÃ§Ã£o de dados
â”‚   â”œâ”€â”€ loaders/          # Carregamento de dados
â”‚   â””â”€â”€ utils/            # UtilitÃ¡rios
â””â”€â”€ tests/                 # Testes
```

## âš™ï¸ Componentes do Pipeline

### 1. ExtraÃ§Ã£o (Extract) ğŸ“¥
- **Responsabilidade**: Coleta dados de fontes externas
- **Tecnologias**: Python, Pandas
- **Componentes**:
  - `src/extractors/data_extractor.py`: ResponsÃ¡vel por ler os arquivos JSON e converter para DataFrames
  - `src/storage/parquet_storage.py`: Gerencia o armazenamento em formato Parquet
- **Artefatos**: Arquivos JSON
- **Output**: Camada Raw

### 2. ValidaÃ§Ã£o (Validate) âœ…
- **Responsabilidade**: ValidaÃ§Ã£o de schema e qualidade
- **Tecnologias**: Pydantic, Pandas
- **Componentes**:
  - `src/validators/data_validator.py`: Valida os dados contra schemas Pydantic
  - `src/validators/data_quality.py`: Realiza validaÃ§Ãµes de qualidade
  - `src/utils/schemas.py`: Define os schemas Pydantic
- **ValidaÃ§Ãµes**:
  - Schema (tipos, campos obrigatÃ³rios)
  - Valores nulos
  - Valores Ãºnicos
  - Formato de dados
- **Output**: Camada Bronze

### 3. TransformaÃ§Ã£o (Transform) ğŸ”„
- **Responsabilidade**: TransformaÃ§Ã£o e enriquecimento
- **Tecnologias**: Pandas
- **Componentes**:
  - `src/transformers/data_transformer.py`: Aplica transformaÃ§Ãµes nos dados
  - `src/utils/schemas.py`: Define os schemas para validaÃ§Ã£o pÃ³s-transformacao
- **TransformaÃ§Ãµes**:
  - NormalizaÃ§Ã£o de campos
  - CÃ¡lculo de mÃ©tricas
  - Enriquecimento de dados
  - Limpeza e padronizaÃ§Ã£o
- **Output**: Camada Silver

### 4. Carregamento (Load) ğŸ“¤
- **Responsabilidade**: Carregamento no banco de dados
- **Tecnologias**: SQLAlchemy, PostgreSQL
- **Componentes**:
  - `src/loaders/gold_loader.py`: Gerencia o carregamento para o PostgreSQL
- **OperaÃ§Ãµes**:
  - Upsert de dados
  - AtualizaÃ§Ã£o de views
  - ManutenÃ§Ã£o de Ã­ndices
- **Output**: Camada Gold

### 5. OrquestraÃ§Ã£o ğŸ­
- **Responsabilidade**: Coordenar a execuÃ§Ã£o da pipeline
- **Componentes**:
  - `dags/caf_pipeline.py`: Define o DAG do Airflow
  - `src/utils/metadata.py`: Gerencia metadados da execuÃ§Ã£o
- **ConfiguraÃ§Ã£o**:
  - `docker-compose.yaml`: ConfiguraÃ§Ã£o dos serviÃ§os
  - `Dockerfile`: ConfiguraÃ§Ã£o do container Airflow
  - `entrypoint.sh`: Script de inicializaÃ§Ã£o
  - `requirements.txt`: DependÃªncias Python

## ğŸ“Š Modelo de Dados

### Schema Star â­
O modelo de dados segue o padrÃ£o Star Schema, com:

1. **Tabelas Fato** ğŸ“ˆ
   - `gold.transactions`: Registros de transaÃ§Ãµes
   - `gold.fraud_analysis`: AnÃ¡lise de fraudes
   - `gold.user_behavior`: Comportamento de usuÃ¡rios

2. **Tabelas DimensÃ£o** ğŸ“
   - `gold.users`: Dados de usuÃ¡rios
   - `gold.customers`: Dados de clientes

### Views Materializadas ğŸ‘€

1. **gold.fraud_analysis** ğŸ•µï¸
   - AnÃ¡lise de transaÃ§Ãµes fraudulentas
   - MÃ©tricas de risco
   - PadrÃµes de fraude

2. **gold.user_behavior** ğŸ‘¤
   - Comportamento de usuÃ¡rios
   - PadrÃµes de uso
   - MÃ©tricas de engajamento

## âœ… ValidaÃ§Ãµes de Dados

### 1. User ğŸ‘¤
- **ValidaÃ§Ã£o de Email**:
  - Regex: `^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$`
- **Campos ObrigatÃ³rios**:
  - `_id`
  - `name`
  - `email`
  - `createdAt`
  - `birthdate`

### 2. Customer ğŸ¢
- **ValidaÃ§Ã£o de CNPJ**:
  - Verifica se tem exatamente 14 dÃ­gitos
  - Verifica se contÃ©m apenas nÃºmeros
- **Campos ObrigatÃ³rios**:
  - `_id`
  - `fantasyName`
  - `cnpj`
  - `status`
  - `segment`

### 3. Transaction ğŸ’°
- **Campos ObrigatÃ³rios**:
  - `_id`
  - `tenantId`
  - `userId`
  - `createdAt`
  - `updatedAt`
  - `favoriteFruit`
  - `isFraud`
  - `document`
- **ValidaÃ§Ã£o de Documento**:
  - Garante que Ã© um dicionÃ¡rio vÃ¡lido

## ğŸ“ Metadados

O pipeline mantÃ©m metadados detalhados:

1. **ExecuÃ§Ã£o** â±ï¸
   - Timestamp
   - Status
   - DuraÃ§Ã£o
   - Erros
   - **Arquivos**: `data/metadata/run_[uuid].json`

2. **Dados** ğŸ“Š
   - Contagem de registros
   - EstatÃ­sticas
   - Qualidade
   - TransformaÃ§Ãµes aplicadas

## ğŸ› ï¸ Tecnologias

- **OrquestraÃ§Ã£o**: Apache Airflow
- **Processamento**: Python, Pandas
- **Armazenamento**: Parquet, PostgreSQL
- **ValidaÃ§Ã£o**: Pydantic
- **Infraestrutura**: Docker, Docker Compose

## ğŸš€ ExecuÃ§Ã£o

1. **Setup Inicial**
```bash
docker-compose up -d
```

2. **Acesso ao Airflow** ğŸŒ
- URL: http://localhost:8080
- UsuÃ¡rio: airflow
- Senha: airflow

3. **Monitoramento** ğŸ“Š
- Logs no Airflow
- Metadados em `data/metadata`
- MÃ©tricas no PostgreSQL

## ğŸ”§ ManutenÃ§Ã£o

1. **Backup** ğŸ’¾
   - Dados em Parquet
   - Metadados em JSON
   - Scripts de recuperaÃ§Ã£o

2. **Monitoramento** ğŸ‘€
   - Logs do Airflow
   - MÃ©tricas do PostgreSQL
   - Alertas de erro

3. **Escalabilidade** ğŸ“ˆ
   - Processamento distribuÃ­do
   - Particionamento de dados
   - OtimizaÃ§Ã£o de queries

