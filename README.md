# üöÄ Pipeline de Dados - CAF

Este projeto implementa um pipeline de dados utilizando a arquitetura Medallion, com processamento em camadas (raw, bronze, silver e gold) e orquestra√ß√£o via Apache Airflow.

## üîÑ Fluxo de Dados

1. üì• Os dados s√£o extra√≠dos dos arquivos JSON e salvos na camada raw
2. ‚úÖ Os dados s√£o validados e movidos para a camada bronze
3. üîÑ As transforma√ß√µes s√£o aplicadas e os dados s√£o salvos na camada silver
4. üì§ Os dados s√£o carregados no PostgreSQL na camada gold
5. üé≠ Todo o processo √© orquestrado pelo Airflow
6. üìä Metadados s√£o gerados e armazenados em cada etapa

## üèóÔ∏è Arquitetura

### üìä Diagrama do Pipeline

```mermaid
graph TD
    A[Extract] --> B[Raw Layer]
    B --> C[Validate]
    C --> D[Bronze Layer]
    D --> E[Transform]
    E --> F[Silver Layer]
    F --> G[Load]
    G --> H[Gold Layer]
    H --> I[Analytics Views]
```

### üè∫ Camadas do Medallion

1. **Raw Layer** üóÉÔ∏è
   - Primeira camada de ingest√£o
   - Armazena dados brutos sem transforma√ß√µes
   - Formato: Parquet
   - Mant√©m a estrutura original dos dados
   - Objetivo: Preservar dados originais para reprocessamento
   - **Arquivos de entrada**:
     - `data/users.json`
     - `data/customers.json`
     - `data/transactions.json`
   - **Arquivos de sa√≠da**:
     - `data/parquet/raw/users_[timestamp].parquet`
     - `data/parquet/raw/customers_[timestamp].parquet`
     - `data/parquet/raw/transactions_[timestamp].parquet`

2. **Bronze Layer** üèÜ
   - Dados validados e limpos
   - Valida√ß√£o de schema usando Pydantic
   - Verifica√ß√£o de qualidade de dados
   - Formato: Parquet
   - Objetivo: Garantir integridade dos dados
   - **Arquivos de entrada**: Arquivos Parquet da camada raw
   - **Arquivos de sa√≠da**:
     - `data/parquet/bronze/users_[timestamp].parquet`
     - `data/parquet/bronze/customers_[timestamp].parquet`
     - `data/parquet/bronze/transactions_[timestamp].parquet`

3. **Silver Layer** ü•à
   - Dados transformados e enriquecidos
   - Transforma√ß√µes de neg√≥cio
   - Normaliza√ß√£o de dados
   - Formato: Parquet
   - Objetivo: Preparar dados para an√°lise
   - **Arquivos de entrada**: Arquivos Parquet da camada bronze
   - **Arquivos de sa√≠da**:
     - `data/parquet/silver/users_[timestamp].parquet`
     - `data/parquet/silver/customers_[timestamp].parquet`
     - `data/parquet/silver/transactions_[timestamp].parquet`

4. **Gold Layer** ü•á
   - Dados prontos para consumo
   - Modelo dimensional (Star Schema)
   - Views materializadas para an√°lise
   - Formato: PostgreSQL
   - Objetivo: Suportar an√°lises e relat√≥rios
   - **Arquivos de entrada**: Arquivos Parquet da camada silver
   - **Arquivos de sa√≠da**:
     - Tabelas no PostgreSQL:
       - `gold.users`
       - `gold.customers`
       - `gold.transactions`

## üìÅ Estrutura do Projeto

```
.
‚îú‚îÄ‚îÄ dags/                    # DAGs do Airflow
‚îú‚îÄ‚îÄ data/                    # Dados processados
‚îÇ   ‚îú‚îÄ‚îÄ parquet/            # Arquivos Parquet
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ raw/           # Camada Raw
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bronze/        # Camada Bronze
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ silver/        # Camada Silver
‚îÇ   ‚îî‚îÄ‚îÄ metadata/          # Metadados do pipeline
‚îú‚îÄ‚îÄ src/                    # C√≥digo fonte
‚îÇ   ‚îú‚îÄ‚îÄ extractors/        # Extra√ß√£o de dados
‚îÇ   ‚îú‚îÄ‚îÄ transformers/      # Transforma√ß√£o de dados
‚îÇ   ‚îú‚îÄ‚îÄ validators/        # Valida√ß√£o de dados
‚îÇ   ‚îú‚îÄ‚îÄ loaders/          # Carregamento de dados
‚îÇ   ‚îî‚îÄ‚îÄ utils/            # Utilit√°rios
‚îî‚îÄ‚îÄ tests/                 # Testes
```

## ‚öôÔ∏è Componentes do Pipeline

### 1. Extra√ß√£o (Extract) üì•
- **Responsabilidade**: Coleta dados de fontes externas
- **Tecnologias**: Python, Pandas
- **Componentes**:
  - `src/extractors/data_extractor.py`: Respons√°vel por ler os arquivos JSON e converter para DataFrames
  - `src/storage/parquet_storage.py`: Gerencia o armazenamento em formato Parquet
- **Artefatos**: Arquivos JSON
- **Output**: Camada Raw

### 2. Valida√ß√£o (Validate) ‚úÖ
- **Responsabilidade**: Valida√ß√£o de schema e qualidade
- **Tecnologias**: Pydantic, Pandas
- **Componentes**:
  - `src/validators/data_validator.py`: Valida os dados contra schemas Pydantic
  - `src/validators/data_quality.py`: Realiza valida√ß√µes de qualidade
  - `src/utils/schemas.py`: Define os schemas Pydantic
- **Valida√ß√µes**:
  - Schema (tipos, campos obrigat√≥rios)
  - Valores nulos
  - Valores √∫nicos
  - Formato de dados
- **Output**: Camada Bronze

### 3. Transforma√ß√£o (Transform) üîÑ
- **Responsabilidade**: Transforma√ß√£o e enriquecimento
- **Tecnologias**: Pandas
- **Componentes**:
  - `src/transformers/data_transformer.py`: Aplica transforma√ß√µes nos dados
  - `src/utils/schemas.py`: Define os schemas para valida√ß√£o p√≥s-transformacao
- **Transforma√ß√µes**:
  - Normaliza√ß√£o de campos
  - C√°lculo de m√©tricas
  - Enriquecimento de dados
  - Limpeza e padroniza√ß√£o
- **Output**: Camada Silver

### 4. Carregamento (Load) üì§
- **Responsabilidade**: Carregamento no banco de dados
- **Tecnologias**: SQLAlchemy, PostgreSQL
- **Componentes**:
  - `src/loaders/gold_loader.py`: Gerencia o carregamento para o PostgreSQL
- **Opera√ß√µes**:
  - Upsert de dados
  - Atualiza√ß√£o de views
  - Manuten√ß√£o de √≠ndices
- **Output**: Camada Gold

### 5. Orquestra√ß√£o üé≠
- **Responsabilidade**: Coordenar a execu√ß√£o da pipeline
- **Componentes**:
  - `dags/caf_pipeline.py`: Define o DAG do Airflow
  - `src/utils/metadata.py`: Gerencia metadados da execu√ß√£o
- **Configura√ß√£o**:
  - `docker-compose.yaml`: Configura√ß√£o dos servi√ßos
  - `Dockerfile`: Configura√ß√£o do container Airflow
  - `entrypoint.sh`: Script de inicializa√ß√£o
  - `requirements.txt`: Depend√™ncias Python

## üìä Modelo de Dados

### Schema Star ‚≠ê
O modelo de dados segue o padr√£o Star Schema, com:

1. **Tabelas Fato** üìà
   - `gold.transactions`: Registros de transa√ß√µes
   - `gold.fraud_analysis`: An√°lise de fraudes
   - `gold.user_behavior`: Comportamento de usu√°rios

2. **Tabelas Dimens√£o** üìê
   - `gold.users`: Dados de usu√°rios
   - `gold.customers`: Dados de clientes

### Views Materializadas üëÄ

1. **gold.fraud_analysis** üïµÔ∏è
   - An√°lise de transa√ß√µes fraudulentas
   - M√©tricas de risco
   - Padr√µes de fraude

2. **gold.user_behavior** üë§
   - Comportamento de usu√°rios
   - Padr√µes de uso
   - M√©tricas de engajamento

## ‚úÖ Valida√ß√µes de Dados

### 1. User üë§
- **Valida√ß√£o de Email**:
  - Regex: `^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$`
- **Campos Obrigat√≥rios**:
  - `_id`
  - `name`
  - `email`
  - `createdAt`
  - `birthdate`

### 2. Customer üè¢
- **Valida√ß√£o de CNPJ**:
  - Verifica se tem exatamente 14 d√≠gitos
  - Verifica se cont√©m apenas n√∫meros
- **Campos Obrigat√≥rios**:
  - `_id`
  - `fantasyName`
  - `cnpj`
  - `status`
  - `segment`

### 3. Transaction üí∞
- **Campos Obrigat√≥rios**:
  - `_id`
  - `tenantId`
  - `userId`
  - `createdAt`
  - `updatedAt`
  - `favoriteFruit`
  - `isFraud`
  - `document`
- **Valida√ß√£o de Documento**:
  - Garante que √© um dicion√°rio v√°lido

## üìù Metadados

O pipeline mant√©m metadados detalhados:

1. **Execu√ß√£o** ‚è±Ô∏è
   - Timestamp
   - Status
   - Dura√ß√£o
   - Erros
   - **Arquivos**: `data/metadata/run_[uuid].json`

2. **Dados** üìä
   - Contagem de registros
   - Estat√≠sticas
   - Qualidade
   - Transforma√ß√µes aplicadas

## üõ†Ô∏è Tecnologias

- **Orquestra√ß√£o**: Apache Airflow
- **Processamento**: Python, Pandas
- **Armazenamento**: Parquet, PostgreSQL
- **Valida√ß√£o**: Pydantic
- **Infraestrutura**: Docker, Docker Compose

## üöÄ Execu√ß√£o

1. **Setup Inicial**
```bash
docker-compose up -d
```

2. **Acesso ao Airflow** üåê
- URL: http://localhost:8080
- Usu√°rio: airflow
- Senha: airflow

3. **Monitoramento** üìä
- Logs no Airflow
- Metadados em `data/metadata`
- M√©tricas no PostgreSQL

## üîß Manuten√ß√£o

1. **Backup** üíæ
   - Dados em Parquet
   - Metadados em JSON
   - Scripts de recupera√ß√£o

2. **Monitoramento** üëÄ
   - Logs do Airflow
   - M√©tricas do PostgreSQL
   - Alertas de erro

3. **Escalabilidade** üìà
   - Processamento distribu√≠do
   - Particionamento de dados
   - Otimiza√ß√£o de queries

## CI/CD Pipeline

O projeto utiliza GitHub Actions para automatizar o processo de integra√ß√£o e entrega cont√≠nua (CI/CD). O pipeline √© executado automaticamente em pushes e pull requests para a branch main.

### Workflow

O pipeline consiste em dois jobs principais:

1. **Verifica√ß√£o de Qualidade de C√≥digo**
   - Configura ambiente Python 3.11
   - Instala depend√™ncias
   - Executa linting com flake8 para verificar:
     - Erros de sintaxe
     - Estilo de c√≥digo
     - Complexidade do c√≥digo

2. **Build e Deploy Docker**
   - Constr√≥i a imagem Docker
   - Faz push da imagem para Docker Hub (apenas na branch main)

### Configura√ß√£o

Para que o pipeline funcione corretamente, √© necess√°rio configurar os seguintes secrets no GitHub:

1. Acesse Settings > Secrets and variables > Actions
2. Adicione os secrets:
   - `DOCKERHUB_USERNAME`: Seu nome de usu√°rio no Docker Hub
   - `DOCKERHUB_TOKEN`: Seu token de acesso ao Docker Hub (com permiss√£o Read & Write)

### Execu√ß√£o Local

Para executar as mesmas verifica√ß√µes localmente:

```bash
# Instalar depend√™ncias de desenvolvimento
pip install flake8

# Executar linting
flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
```

### Status do Pipeline

O status do pipeline pode ser verificado na aba "Actions" do reposit√≥rio no GitHub.

